{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffb4715-06cc-4a68-97fc-e818322630f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. The relationship between polynomial functions and kernel functions in machine learning algorithms lies in their use as basis functions to transform data into higher-dimensional spaces. Polynomial functions are a specific type of kernel function used to compute the inner product of the transformed feature vectors. In SVMs, for example, polynomial kernel functions allow the algorithm to create nonlinear decision boundaries by implicitly mapping the input features into a higher-dimensional space using polynomial functions. This mapping enables the SVM to find a hyperplane in the transformed space that separates the classes.\n",
    "\n",
    "# Q2. We can implement an SVM with a polynomial kernel in Python using Scikit-learn by specifying the kernel parameter of the SVC class as 'poly'. Additionally, we can specify the degree of the polynomial using the degree parameter.\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create an instance of the SVC classifier with a polynomial kernel\n",
    "svm_poly = SVC(kernel='poly', degree=3)  # degree=3 is the default degree for polynomial kernel\n",
    "svm_poly.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = svm_poly.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "# Q3. Increasing the value of epsilon in Support Vector Regression (SVR) typically results in a larger margin around the predicted function. As a result, more data points may fall within this margin, leading to fewer support vectors. Conversely, decreasing epsilon may result in a smaller margin, potentially encompassing fewer data points and requiring more support vectors to define the predicted function accurately.\n",
    "\n",
    "# Q4. The performance of Support Vector Regression (SVR) is influenced by several parameters:\n",
    "   - Kernel function: Different kernel functions, such as linear, polynomial, and radial basis function (RBF), can capture different types of relationships in the data. The choice of kernel function affects the flexibility of the SVR model.\n",
    "   - C parameter: The regularization parameter C controls the trade-off between maximizing the margin and minimizing the training error. Higher values of C allow for fewer margin violations (soft margin), potentially leading to overfitting, while lower values of C prioritize a wider margin and may result in underfitting.\n",
    "   - Epsilon parameter: Epsilon (ε) determines the width of the margin around the predicted function. Larger values of epsilon result in a wider margin, potentially encompassing more data points within the margin. Smaller values of epsilon result in a narrower margin and may lead to fewer support vectors.\n",
    "   - Gamma parameter: For kernel functions like RBF, gamma (γ) defines the influence of individual training samples on the decision boundary. Higher values of gamma result in a more complex decision boundary, potentially leading to overfitting, while lower values of gamma result in a smoother decision boundary.\n",
    "\n",
    "   Example scenarios:\n",
    "   - Increase C: When the training data contains noise or outliers, increasing C can help reduce the influence of these points and improve generalization.\n",
    "   - Decrease C: When the dataset is large and noisy, reducing C can prevent overfitting by allowing for a wider margin.\n",
    "   - Increase epsilon: When the prediction task allows for some deviation from the target values, increasing epsilon can lead to a more robust model.\n",
    "   - Adjust gamma: Higher gamma values may be suitable for datasets with complex relationships, while lower gamma values may be preferable for smoother decision boundaries.\n",
    "\n",
    "# Q5. **Assignment:**\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "svm = SVC()\n",
    "\n",
    "# Tune hyperparameters using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1], 'kernel': ['rbf', 'poly', 'sigmoid']}\n",
    "grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "best_svm = SVC(**best_params)\n",
    "best_svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Save the trained classifier to a file\n",
    "joblib.dump(best_svm, 'svm_classifier.pkl')\n",
    "\n",
    "# Load the saved classifier\n",
    "loaded_svm = joblib.load('svm_classifier.pkl')\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "y_pred_test = loaded_svm.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
