{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3ea89a-add0-4ec9-b40e-c5f9a929fa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "\n",
    "# Q1. How does bagging reduce overfitting in decision trees?\n",
    "# Bagging (Bootstrap Aggregating) reduces overfitting in decision trees by averaging the predictions of multiple decision trees trained on different bootstrap samples of the data. Each bootstrap sample is created by randomly sampling the original dataset with replacement, resulting in different subsets of the data for each tree. This process introduces diversity among the trees and helps to cancel out the individual overfitting tendencies of the decision trees, leading to a more robust and generalized model.\n",
    "\n",
    "# Q2. What are the advantages and disadvantages of using different types of base learners in bagging?\n",
    "# Advantages:\n",
    "# - Using different types of base learners can increase the diversity among the models, which can improve the overall performance of the ensemble.\n",
    "# - Different base learners can capture different patterns and relationships in the data, leading to a more comprehensive model.\n",
    "\n",
    "# Disadvantages:\n",
    "# - Combining different types of base learners can increase the complexity of the model and make it more difficult to interpret.\n",
    "# - The computational cost and time required for training may increase, especially if the base learners have different training requirements.\n",
    "\n",
    "# Q3. How does the choice of base learner affect the bias-variance tradeoff in bagging?\n",
    "# The choice of base learner affects the bias-variance tradeoff in bagging as follows:\n",
    "# - Low-bias, high-variance base learners (e.g., decision trees) benefit more from bagging because averaging their predictions reduces variance without significantly increasing bias.\n",
    "# - High-bias, low-variance base learners (e.g., linear models) may not benefit as much from bagging because their high bias limits the improvement in predictive performance.\n",
    "\n",
    "# Q4. Can bagging be used for both classification and regression tasks? How does it differ in each case?\n",
    "# Yes, bagging can be used for both classification and regression tasks.\n",
    "# - For classification tasks, bagging typically uses majority voting to combine the predictions of the base learners. Each base learner votes for a class label, and the class with the most votes is the final prediction.\n",
    "# - For regression tasks, bagging uses averaging to combine the predictions of the base learners. The final prediction is the average of the predictions of the base learners.\n",
    "\n",
    "# Q5. What is the role of ensemble size in bagging? How many models should be included in the ensemble?\n",
    "# The ensemble size (number of base learners) in bagging plays a crucial role in determining the performance of the model:\n",
    "# - A larger ensemble size generally leads to better performance by reducing variance and increasing stability.\n",
    "# - However, the improvement in performance may plateau after a certain point, and additional base learners may not provide significant benefits.\n",
    "# - The optimal ensemble size depends on the complexity of the problem, the computational resources available, and the diversity of the base learners.\n",
    "\n",
    "# Q6. Can you provide an example of a real-world application of bagging in machine learning?\n",
    "# Bagging is commonly used in various real-world applications. One example is its use in Random Forests for classification and regression tasks.\n",
    "# In the finance industry, bagging (Random Forests) is used for credit scoring and risk assessment. The ensemble model can predict the likelihood of a borrower defaulting on a loan by combining the predictions of multiple decision trees trained on historical credit data.\n",
    "\n",
    "# Example: Estimating 95% confidence interval for the population mean height using bootstrap\n",
    "# Given data\n",
    "sample_mean = 15\n",
    "sample_std = 2\n",
    "sample_size = 50\n",
    "confidence_level = 0.95\n",
    "\n",
    "# Calculate standard error of the mean (SEM)\n",
    "SEM = sample_std / np.sqrt(sample_size)\n",
    "\n",
    "# Calculate margin of error\n",
    "margin_of_error = 1.96 * SEM  # For 95% confidence interval\n",
    "\n",
    "# Calculate lower and upper bounds of the confidence interval\n",
    "lower_bound = sample_mean - margin_of_error\n",
    "upper_bound = sample_mean + margin_of_error\n",
    "\n",
    "print(\"95% Confidence Interval for the population mean height:\")\n",
    "print(\"Lower Bound:\", lower_bound)\n",
    "print(\"Upper Bound:\", upper_bound)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
