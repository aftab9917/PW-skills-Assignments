{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "296c606a-1c8f-4767-b0b3-7293c619a03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER 1\n",
    "# The Filter method in feature selection is a technique where features are selected based on their statistical properties without involving any machine learning algorithm. It works by evaluating the characteristics of each feature independently of the target variable. Common metrics used in the Filter method include correlation, mutual information, chi-square, and ANOVA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6a737fc-6a43-4063-b66c-ea8499b1f1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER 2\n",
    "# The Wrapper method differs from the Filter method in that it evaluates feature subsets based on their performance using a specific machine learning algorithm. It selects features by iterating through different combinations of features, training the model on each subset, and selecting the subset that yields the best performance according to a predefined criterion, such as accuracy or cross-validation score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a318a1e9-16eb-4c8f-b2d5-cbb20eb36dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER 3\n",
    "# Some common techniques used in Embedded feature selection methods include LASSO (Least Absolute Shrinkage and Selection Operator), Ridge Regression, Elastic Net, and Decision Trees. These methods incorporate feature selection within the model training process, penalizing coefficients of irrelevant features or selecting features based on their importance during model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c6a0b04-d179-4d54-a958-48d23b9fd73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER 4\n",
    "# Drawbacks of using the Filter method for feature selection include:\n",
    "# - It does not consider the interaction between features.\n",
    "# - It may eliminate relevant features if they are not highly correlated with the target variable.\n",
    "# - It does not take into account the predictive power of features in combination with each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6696b930-27ac-453b-b520-a433c1d4c1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER 5\n",
    "# The Filter method is preferred over the Wrapper method for feature selection in situations where:\n",
    "# - There is a large number of features relative to the number of samples.\n",
    "# - Computation resources are limited, as the Filter method is computationally less intensive compared to the Wrapper method.\n",
    "# - There is a need for a quick and simple feature selection process without fine-tuning model parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a1b0d17-b550-4820-881e-65a6252af22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER 6\n",
    "# In the telecom company's project to predict customer churn, I would use the Filter method to choose the most pertinent attributes by:\n",
    "# - Calculating correlation coefficients between each feature and the target variable (churn).\n",
    "# - Selecting the features with the highest absolute correlation values, indicating a strong linear relationship with churn.\n",
    "# - Conducting further analysis using mutual information or chi-square tests to identify non-linear relationships or dependencies between features and churn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5dbcd4b-642e-4709-9730-c4613b1b71f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER 7\n",
    "# In the soccer match outcome prediction project, I would use the Embedded method to select the most relevant features by:\n",
    "# - Training a predictive model (e.g., logistic regression, random forest) on the entire dataset, including player statistics and team rankings.\n",
    "# - Evaluating the importance of each feature based on the model's coefficients or feature importances.\n",
    "# - Selecting features with significant coefficients or high importances as they contribute most to predicting the outcome of soccer matches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705a0bbe-4a74-4218-9b4f-08124bd20a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER 8\n",
    "# In the project to predict house prices, I would use the Wrapper method to select the best set of features by:\n",
    "# - Initially, selecting a subset of features based on domain knowledge or intuition.\n",
    "# - Iteratively training the model on different combinations of features using techniques like forward selection, backward elimination, or recursive feature elimination.\n",
    "# - Evaluating the performance of each feature subset using a performance metric such as mean squared error (MSE) or R-squared.\n",
    "# - Selecting the feature subset that yields the lowest prediction error as the final set of features for the house price predictor.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
