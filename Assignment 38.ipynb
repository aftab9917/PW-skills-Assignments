{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6baf1a8-882c-40b7-90c1-3da75790eed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER 1\n",
    "# Min-Max scaling is a data preprocessing technique used to scale numeric features to a specific range, typically [0, 1]. It works by subtracting the minimum value from each observation and then dividing by the range of the feature (the maximum value minus the minimum value). This technique preserves the original distribution of the data while ensuring that all features have the same scale. Min-Max scaling is given by the formula:\n",
    "# X_scaled = (X - X_min) / (X_max - X_min)\n",
    "# where X is the original feature value, X_min is the minimum value of the feature, and X_max is the maximum value of the feature.\n",
    "\n",
    "# Example:\n",
    "# Suppose we have a feature 'age' with values ranging from 20 to 60. To apply Min-Max scaling, we subtract the minimum value (20) from each observation and then divide by the range (60 - 20 = 40).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fdc0fb0-ef97-4bc0-ac5d-8dec32805954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER 2\n",
    "# The Unit Vector technique in feature scaling scales each feature to have unit norm, i.e., it transforms the data points onto a unit hypersphere. It differs from Min-Max scaling in that it doesn't necessarily bound the data within a specific range like [0, 1]. Instead, it ensures that each data point lies on the unit hypersphere, preserving the direction of the data while normalizing its magnitude.\n",
    "\n",
    "# Example:\n",
    "# Suppose we have a dataset with two features: 'height' and 'weight'. To apply the Unit Vector technique, we first compute the norm (magnitude) of each data point in the dataset. Then, we divide each feature by its corresponding norm, ensuring that the transformed data points lie on the unit hypersphere.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6450ed7b-dcd5-47d4-b414-596ce5e03b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER 3\n",
    "# PCA (Principal Component Analysis) is a dimensionality reduction technique used to reduce the number of features in a dataset while preserving most of its variance. It works by transforming the original features into a new set of orthogonal (uncorrelated) features called principal components. These principal components are ordered by the amount of variance they explain in the data, with the first component capturing the most variance.\n",
    "\n",
    "# Example:\n",
    "# Suppose we have a dataset with multiple correlated features such as 'height', 'weight', and 'age'. PCA can be applied to this dataset to find the principal components that capture the most variation in the data. The resulting principal components can then be used as new features to represent the data in a lower-dimensional space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d029cf59-e7b7-4d05-a788-7a481e00af12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER 4\n",
    "# PCA and Feature Extraction are closely related concepts. PCA can be used for Feature Extraction by transforming the original features into a new set of principal components. These principal components are linear combinations of the original features and are chosen to capture the maximum variance in the data. By selecting a subset of the top principal components, we effectively perform feature extraction, reducing the dimensionality of the dataset while preserving most of its information.\n",
    "\n",
    "# Example:\n",
    "# Suppose we have a dataset with high-dimensional features such as pixel intensities of images. We can apply PCA to extract the most informative features (principal components) from the original pixel intensities. These principal components represent patterns or structures in the images, allowing us to reduce the dimensionality of the dataset while retaining important information for tasks such as image classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f0a275d-cec7-4ff6-8ec8-59dfef364d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER 5\n",
    "# In the food delivery recommendation system project, Min-Max scaling can be used to preprocess the data as follows:\n",
    "# - Calculate the minimum and maximum values for each feature (e.g., price, rating, delivery time).\n",
    "# - For each feature, apply the Min-Max scaling formula to scale the values to the range [0, 1].\n",
    "# - The scaled features will now have the same scale, allowing for fair comparison and preventing features with larger magnitudes from dominating the model's learning process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c7fb914-37d4-46ca-b104-c689ac886fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER 6\n",
    "# In the stock price prediction project, PCA can be used to reduce the dimensionality of the dataset as follows:\n",
    "# - Normalize the features to have zero mean and unit variance.\n",
    "# - Apply PCA to the normalized dataset to find the principal components.\n",
    "# - Select a subset of the principal components that capture most of the variance in the data.\n",
    "# - Project the original dataset onto the selected principal components, effectively reducing its dimensionality while retaining most of the information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17b8e751-f94e-429b-9fdd-96dbcabef370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER 7\n",
    "# For the given dataset [1, 5, 10, 15, 20], we perform Min-Max scaling as follows:\n",
    "# - The minimum value (X_min) is 1, and the maximum value (X_max) is 20.\n",
    "# - Using the Min-Max scaling formula, we transform each value to the range [-1, 1].\n",
    "# - The scaled values are calculated as follows:\n",
    "#   - (1 - 1) / (20 - 1) = 0\n",
    "#   - (5 - 1) / (20 - 1) = 0.1667\n",
    "#   - (10 - 1) / (20 - 1) = 0.4444\n",
    "#   - (15 - 1) / (20 - 1) = 0.7222\n",
    "#   - (20 - 1) / (20 - 1) = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5203a3-02e1-4717-90c2-361b45dae6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER 8\n",
    "# For the dataset containing features [height, weight, age, gender, blood pressure], we perform Feature Extraction using PCA as follows:\n",
    "# - Normalize the features to have zero mean and unit variance.\n",
    "# - Apply PCA to the normalized dataset to find the principal components.\n",
    "# - Evaluate the explained variance ratio for each principal component.\n",
    "# - Choose the number of principal components to retain based on a predefined threshold (e.g., retaining components that explain 95% of the variance).\n",
    "# - The number of principal components chosen for retention depends on the trade-off between dimensionality reduction and preserving information.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
