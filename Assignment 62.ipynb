{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a24f6b-3ed6-4289-95d2-3424b8eab833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is Random Forest Regressor?\n",
    "# A Random Forest Regressor is an ensemble learning method that uses multiple decision trees to make predictions for regression tasks. It combines the predictions of several base decision trees, each built on different subsets of the training data and features, to produce a final aggregated prediction. This approach aims to improve the predictive performance and robustness of individual decision trees.\n",
    "\n",
    "# Q2. How does Random Forest Regressor reduce the risk of overfitting?\n",
    "# Random Forest Regressor reduces the risk of overfitting by:\n",
    "# - Training multiple decision trees on different bootstrap samples (random subsets with replacement) of the training data, which introduces diversity among the trees.\n",
    "# - Using random subsets of features for each split in the decision trees, which further diversifies the trees and prevents them from being overly dependent on any particular feature.\n",
    "# The aggregation of predictions from multiple diverse trees helps to average out the errors and reduces the likelihood of overfitting to the training data.\n",
    "\n",
    "# Q3. How does Random Forest Regressor aggregate the predictions of multiple decision trees?\n",
    "# In a Random Forest Regressor, the predictions from multiple decision trees are aggregated by averaging their individual predictions. Each decision tree in the ensemble makes a prediction for a given input, and the final prediction of the random forest is the mean of these individual predictions.\n",
    "\n",
    "# Q4. What are the hyperparameters of Random Forest Regressor?\n",
    "# Some key hyperparameters of Random Forest Regressor include:\n",
    "# - `n_estimators`: The number of decision trees in the forest.\n",
    "# - `max_depth`: The maximum depth of each decision tree.\n",
    "# - `min_samples_split`: The minimum number of samples required to split an internal node.\n",
    "# - `min_samples_leaf`: The minimum number of samples required to be at a leaf node.\n",
    "# - `max_features`: The number of features to consider when looking for the best split.\n",
    "# - `bootstrap`: Whether bootstrap samples are used when building trees.\n",
    "# - `random_state`: Seed used by the random number generator.\n",
    "\n",
    "# Q5. What is the difference between Random Forest Regressor and Decision Tree Regressor?\n",
    "# - A Decision Tree Regressor uses a single decision tree to make predictions, which can lead to overfitting if the tree is too deep or if the data is noisy.\n",
    "# - A Random Forest Regressor uses multiple decision trees trained on different subsets of the data and features, and aggregates their predictions to improve accuracy and robustness. This ensemble approach reduces the risk of overfitting and generally provides better performance than a single decision tree.\n",
    "\n",
    "# Q6. What are the advantages and disadvantages of Random Forest Regressor?\n",
    "# Advantages:\n",
    "# - Reduces overfitting compared to individual decision trees.\n",
    "# - Provides robust and accurate predictions.\n",
    "# - Handles both numerical and categorical features.\n",
    "# - Works well with large datasets and high-dimensional data.\n",
    "\n",
    "# Disadvantages:\n",
    "# - Can be computationally intensive and memory-consuming, especially with a large number of trees.\n",
    "# - The model is less interpretable than a single decision tree.\n",
    "\n",
    "# Q7. What is the output of Random Forest Regressor?\n",
    "# The output of a Random Forest Regressor is a continuous value, which is the average of the predictions made by the individual decision trees in the ensemble for a given input.\n",
    "\n",
    "# Q8. Can Random Forest Regressor be used for classification tasks?\n",
    "# No, Random Forest Regressor is specifically designed for regression tasks where the output is a continuous value. However, the Random Forest algorithm can be adapted for classification tasks using a similar approach called Random Forest Classifier. In classification, the individual decision trees vote for a class label, and the final prediction is based on the majority vote among the trees.\n",
    "\n",
    "# Example of implementing Random Forest Regressor in Python:\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load dataset\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train Random Forest Regressor\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Example of using Random Forest Classifier for classification tasks:\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = rf_classifier.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
