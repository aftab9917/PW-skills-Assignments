{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7765c20e-7a23-4945-a4e1-33b5b444c210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Q1: What is the mathematical formula for a linear SVM?\n",
    "# A linear SVM aims to find a hyperplane that separates the data into two classes. The decision function can be written as:\n",
    "# f(x) = w.T * x + b\n",
    "# where w is the weight vector, x is the input feature vector, and b is the bias term.\n",
    "\n",
    "# Q2: What is the objective function of a linear SVM?\n",
    "# The objective function for a linear SVM is to minimize the following:\n",
    "# min (1/2) * ||w||^2 + C * sum(両_i)\n",
    "# subject to y_i * (w.T * x_i + b) >= 1 - 両_i for all i and 両_i >= 0\n",
    "# where 両_i are the slack variables that allow for misclassifications, and C is the regularization parameter that controls the trade-off between maximizing the margin and minimizing the classification error.\n",
    "\n",
    "# Q3: What is the kernel trick in SVM?\n",
    "# The kernel trick allows SVMs to create nonlinear decision boundaries by implicitly mapping the input features into a higher-dimensional space. Instead of computing the mapping explicitly, the kernel trick computes the inner product of the mapped features directly using a kernel function. Common kernels include the polynomial kernel and the radial basis function (RBF) kernel.\n",
    "\n",
    "# Q4: What is the role of support vectors in SVM? Explain with an example.\n",
    "# Support vectors are the data points that lie closest to the decision boundary (hyperplane). They are the critical elements of the training set because they define the position and orientation of the hyperplane. The SVM algorithm optimizes the margin between the support vectors of the two classes.\n",
    "\n",
    "# Example:\n",
    "# Consider a binary classification problem with two features. The support vectors are the points that lie on the margins or violate them slightly (in the case of soft margin SVM). The hyperplane is adjusted based on these support vectors, ensuring the maximum margin between the classes.\n",
    "\n",
    "# Q5: Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin, and Hard margin in SVM.\n",
    "# We will use synthetic data to illustrate these concepts.\n",
    "\n",
    "# Generate synthetic data\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=100, centers=2, random_state=42, cluster_std=1.0)\n",
    "y = np.where(y == 0, -1, 1)  # Convert labels to -1 and 1 for SVM\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm')\n",
    "plt.title(\"Synthetic Data for SVM Illustration\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()\n",
    "\n",
    "# Train a hard margin SVM\n",
    "svm_hard = SVC(kernel='linear', C=1e5)\n",
    "svm_hard.fit(X, y)\n",
    "\n",
    "# Plot the decision boundary and support vectors\n",
    "def plot_svm(svm, X, y):\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm')\n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    # Create grid to evaluate model\n",
    "    xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 50),\n",
    "                         np.linspace(ylim[0], ylim[1], 50))\n",
    "    xy = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "    Z = svm.decision_function(xy).reshape(xx.shape)\n",
    "\n",
    "    # Plot decision boundary and margins\n",
    "    ax.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    ax.scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1], s=100,\n",
    "               linewidth=1, facecolors='none', edgecolors='k')\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.title(\"SVM Decision Boundary and Margins\")\n",
    "    plt.show()\n",
    "\n",
    "plot_svm(svm_hard, X, y)\n",
    "\n",
    "# Train a soft margin SVM\n",
    "svm_soft = SVC(kernel='linear', C=1.0)\n",
    "svm_soft.fit(X, y)\n",
    "plot_svm(svm_soft, X, y)\n",
    "\n",
    "# Q6: SVM Implementation through Iris dataset\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Use only two classes for binary classification\n",
    "X = X[y != 2]\n",
    "y = y[y != 2]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a linear SVM classifier using scikit-learn\n",
    "svm_clf = SVC(kernel='linear', C=1.0)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of scikit-learn SVM: {accuracy:.2f}\")\n",
    "\n",
    "# Plot the decision boundaries using two features\n",
    "def plot_decision_boundaries(X, y, model, title):\n",
    "    h = .02  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o', s=50, cmap=plt.cm.coolwarm)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.show()\n",
    "\n",
    "# Plot decision boundaries\n",
    "plot_decision_boundaries(X_test[:, :2], y_test, svm_clf, \"SVM Decision Boundaries\")\n",
    "\n",
    "# Try different values of the regularization parameter C\n",
    "C_values = [0.01, 0.1, 1, 10, 100]\n",
    "accuracies = []\n",
    "\n",
    "for C in C_values:\n",
    "    svm_clf = SVC(kernel='linear', C=C)\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "    y_pred = svm_clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(acc)\n",
    "    print(f\"Accuracy with C={C}: {acc:.2f}\")\n",
    "\n",
    "# Plot the effect of C on accuracy\n",
    "plt.plot(C_values, accuracies, marker='o')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Regularization parameter C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Effect of C on SVM Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Bonus Task: Implement a linear SVM classifier from scratch\n",
    "class LinearSVM:\n",
    "    def __init__(self, C=1.0, learning_rate=0.001, n_iters=1000):\n",
    "        self.C = C\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        y_ = np.where(y <= 0, -1, 1)\n",
    "\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
    "                if condition:\n",
    "                    self.w -= self.lr * (2 * self.w / self.n_iters)\n",
    "                else:\n",
    "                    self.w -= self.lr * (2 * self.w / self.n_iters - np.dot(x_i, y_[idx]))\n",
    "                    self.b -= self.lr * y_[idx]\n",
    "\n",
    "    def predict(self, X):\n",
    "        approx = np.dot(X, self.w) - self.b\n",
    "        return np.sign(approx)\n",
    "\n",
    "# Train the custom SVM\n",
    "svm_scratch = LinearSVM(C=1.0, learning_rate=0.001, n_iters=1000)\n",
    "svm_scratch.fit(X_train, y_train)\n",
    "y_pred_scratch = svm_scratch.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy_scratch = accuracy_score(y_test, y_pred_scratch)\n",
    "print(f\"Accuracy of custom SVM: {accuracy_scratch:.2f}\")\n",
    "\n",
    "# Compare with scikit-learn implementation\n",
    "print(f\"Accuracy of scikit-learn SVM: {accuracy:.2f}\")\n",
    "print(f\"Accuracy of custom SVM: {accuracy_scratch:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
