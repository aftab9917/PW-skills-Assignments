{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85efbb7f-26e2-4471-a3ef-3a500912bd44",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 7) (3482848215.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 7\u001b[0;36m\u001b[0m\n\u001b[0;31m    Predicting a person's weight based on their height.\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 7)\n"
     ]
    }
   ],
   "source": [
    "# Q1: Simple Linear Regression vs Multiple Linear Regression\n",
    "\n",
    "**Simple Linear Regression:**\n",
    "Simple linear regression involves one independent variable and one dependent variable. It aims to find the best-fit line that predicts the dependent variable based on the independent variable.\n",
    "\n",
    "*Example:*\n",
    "Predicting a person's weight based on their height.\n",
    "\n",
    "```python\n",
    "# Example of Simple Linear Regression\n",
    "heights = np.array([150, 160, 170, 180, 190]).reshape(-1, 1)  # Heights in cm\n",
    "weights = np.array([50, 60, 65, 70, 80])  # Weights in kg\n",
    "\n",
    "# Create a linear regression model\n",
    "simple_lr = LinearRegression()\n",
    "simple_lr.fit(heights, weights)\n",
    "\n",
    "# Predict weights based on heights\n",
    "predicted_weights = simple_lr.predict(heights)\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(heights, weights, color='blue', label='Actual weights')\n",
    "plt.plot(heights, predicted_weights, color='red', label='Predicted weights')\n",
    "plt.xlabel('Height (cm)')\n",
    "plt.ylabel('Weight (kg)')\n",
    "plt.title('Simple Linear Regression')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eda2dfc2-bb81-4592-b46f-c47eb335398e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example of Multiple Linear Regression\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHeight\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m150\u001b[39m, \u001b[38;5;241m160\u001b[39m, \u001b[38;5;241m170\u001b[39m, \u001b[38;5;241m180\u001b[39m, \u001b[38;5;241m190\u001b[39m],\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m22\u001b[39m, \u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m35\u001b[39m, \u001b[38;5;241m40\u001b[39m],\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeight\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m60\u001b[39m, \u001b[38;5;241m65\u001b[39m, \u001b[38;5;241m70\u001b[39m, \u001b[38;5;241m80\u001b[39m]\n\u001b[1;32m      6\u001b[0m })\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Independent variables (Height and Age)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m X \u001b[38;5;241m=\u001b[39m data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHeight\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Example of Multiple Linear Regression\n",
    "data = pd.DataFrame({\n",
    "    'Height': [150, 160, 170, 180, 190],\n",
    "    'Age': [22, 25, 30, 35, 40],\n",
    "    'Weight': [50, 60, 65, 70, 80]\n",
    "})\n",
    "\n",
    "# Independent variables (Height and Age)\n",
    "X = data[['Height', 'Age']]\n",
    "# Dependent variable (Weight)\n",
    "y = data['Weight']\n",
    "\n",
    "# Create a multiple linear regression model\n",
    "multiple_lr = LinearRegression()\n",
    "multiple_lr.fit(X, y)\n",
    "\n",
    "# Predict weights based on height and age\n",
    "predicted_weights = multiple_lr.predict(X)\n",
    "\n",
    "# Display the results\n",
    "data['Predicted_Weight'] = predicted_weights\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7598306-7a2f-4ce9-921f-13344f2554f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Multiple Linear Regression\n",
    "data = pd.DataFrame({\n",
    "    'Height': [150, 160, 170, 180, 190],\n",
    "    'Age': [22, 25, 30, 35, 40],\n",
    "    'Weight': [50, 60, 65, 70, 80]\n",
    "})\n",
    "\n",
    "# Independent variables (Height and Age)\n",
    "X = data[['Height', 'Age']]\n",
    "# Dependent variable (Weight)\n",
    "y = data['Weight']\n",
    "\n",
    "# Create a multiple linear regression model\n",
    "multiple_lr = LinearRegression()\n",
    "multiple_lr.fit(X, y)\n",
    "\n",
    "# Predict weights based on height and age\n",
    "predicted_weights = multiple_lr.predict(X)\n",
    "\n",
    "# Display the results\n",
    "data['Predicted_Weight'] = predicted_weights\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c077b522-99c2-4d3f-a849-05e4f29dc21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Multiple Linear Regression\n",
    "data = pd.DataFrame({\n",
    "    'Height': [150, 160, 170, 180, 190],\n",
    "    'Age': [22, 25, 30, 35, 40],\n",
    "    'Weight': [50, 60, 65, 70, 80]\n",
    "})\n",
    "\n",
    "# Independent variables (Height and Age)\n",
    "X = data[['Height', 'Age']]\n",
    "# Dependent variable (Weight)\n",
    "y = data['Weight']\n",
    "\n",
    "# Create a multiple linear regression model\n",
    "multiple_lr = LinearRegression()\n",
    "multiple_lr.fit(X, y)\n",
    "\n",
    "# Predict weights based on height and age\n",
    "predicted_weights = multiple_lr.predict(X)\n",
    "\n",
    "# Display the results\n",
    "data['Predicted_Weight'] = predicted_weights\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30cf4a5-3aad-43be-add8-0d368d9c8002",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Q4: Explain the concept of gradient descent. How is it used in machine learning?\n",
    "\n",
    "```markdown\n",
    "# Q4: Gradient Descent\n",
    "\n",
    "**Gradient Descent** is an optimization algorithm used to minimize the cost function in machine learning models, particularly for linear regression.\n",
    "\n",
    "*Concept:*\n",
    "1. **Initialization**: Start with random values for the parameters (e.g., coefficients in linear regression).\n",
    "2. **Compute Gradient**: Calculate the gradient of the cost function with respect to each parameter.\n",
    "3. **Update Parameters**: Adjust the parameters in the opposite direction of the gradient by a certain step size (learning rate).\n",
    "4. **Iterate**: Repeat steps 2 and 3 until the cost function converges to a minimum value.\n",
    "\n",
    "*Usage in Machine Learning:*\n",
    "Gradient descent is used to find the optimal parameters that minimize the cost function, thereby improving the model's accuracy.\n",
    "\n",
    "```python\n",
    "# Example of Gradient Descent for a simple linear regression problem\n",
    "def gradient_descent(X, y, lr=0.01, iterations=1000):\n",
    "    m = len(y)\n",
    "    theta = np.zeros(2)\n",
    "    X_b = np.c_[np.ones((m, 1)), X]  # Add x0 = 1 to each instance\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "        theta = theta - lr * gradients\n",
    "    \n",
    "    return theta\n",
    "\n",
    "# Generate some data\n",
    "np.random.seed(0)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# Perform gradient descent\n",
    "theta = gradient_descent(X, y)\n",
    "print(f\"Intercept: {theta[0]}\")\n",
    "print(f\"Slope: {theta[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78755d20-8d17-439f-914a-0a6fa3266973",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Q5: Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "\n",
    "```markdown\n",
    "# Q5: Multiple Linear Regression Model\n",
    "\n",
    "**Multiple Linear Regression** is an extension of simple linear regression. It involves two or more independent variables to predict a single dependent variable.\n",
    "\n",
    "*Model:*\n",
    "\\[ y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_nx_n + \\epsilon \\]\n",
    "\n",
    "*Difference from Simple Linear Regression:*\n",
    "- **Simple Linear Regression:** Involves one independent variable.\n",
    "- **Multiple Linear Regression:** Involves multiple independent variables.\n",
    "\n",
    "*Example:*\n",
    "Predicting a person's weight based on their height and age.\n",
    "\n",
    "```python\n",
    "# Example of Multiple Linear Regression\n",
    "data = pd.DataFrame({\n",
    "    'Height': [150, 160, 170, 180, 190],\n",
    "    'Age': [22, 25, 30, 35, 40],\n",
    "    'Weight': [50, 60, 65, 70, 80]\n",
    "})\n",
    "\n",
    "# Independent variables (Height and Age)\n",
    "X = data[['Height', 'Age']]\n",
    "# Dependent variable (Weight)\n",
    "y = data['Weight']\n",
    "\n",
    "# Create a multiple linear regression model\n",
    "multiple_lr = LinearRegression()\n",
    "multiple_lr.fit(X, y)\n",
    "\n",
    "# Intercept and Coefficients\n",
    "intercept = multiple_lr.intercept_\n",
    "coefficients = multiple_lr.coef_\n",
    "\n",
    "print(f\"Intercept: {intercept}\")\n",
    "print(f\"Coefficients: {coefficients}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d545fb54-1e4f-4faa-a81c-4b3f1dea7b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Q6: Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
    "\n",
    "```markdown\n",
    "# Q6: Multicollinearity in Multiple Linear Regression\n",
    "\n",
    "**Multicollinearity** occurs when two or more independent variables in a multiple regression model are highly correlated. This can cause problems because it makes it difficult to determine the individual effect of each independent variable on the dependent variable.\n",
    "\n",
    "*Detection:*\n",
    "1. **Correlation Matrix:** Check the correlation coefficients between pairs of independent variables.\n",
    "2. **Variance Inflation Factor (VIF):** Calculate VIF for each independent variable. A VIF value greater than 5 or 10 indicates high multicollinearity.\n",
    "\n",
    "*Addressing Multicollinearity:*\n",
    "1. **Remove Highly Correlated Variables:** Drop one of the correlated variables.\n",
    "2. **Combine Variables:** Combine correlated variables into a single predictor.\n",
    "3. **Principal Component Analysis (PCA):** Use PCA to transform correlated variables into a set of uncorrelated components.\n",
    "\n",
    "```python\n",
    "# Example: Detecting multicollinearity using VIF\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Create a sample dataset\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 3)\n",
    "X[:, 1] = X[:, 0] + np.random.normal(scale=0.1, size=100)  # Create high correlation\n",
    "y = 3*X[:, 0] + 5*X[:, 1] + np.random.randn(100)\n",
    "\n",
    "# Calculate VIF for each independent variable\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = ['X1', 'X2', 'X3']\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X, i) for i in range(X.shape[1])]\n",
    "print(vif_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962adbdc-1ba7-42f8-a3ee-90fd0bd43b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Q7: Describe the polynomial regression model. How is it different from linear regression?\n",
    "\n",
    "```markdown\n",
    "# Q7: Polynomial Regression Model\n",
    "\n",
    "**Polynomial Regression** is a type of regression analysis where the relationship between the independent variable \\( x \\) and the dependent variable \\( y \\) is modeled as an \\( n \\)-th degree polynomial.\n",
    "\n",
    "*Model:*\n",
    "\\[ y = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\ldots + \\beta_nx^n + \\epsilon \\]\n",
    "\n",
    "*Difference from Linear Regression:*\n",
    "- **Linear Regression:** Models a linear relationship between \\( x \\) and \\( y \\).\n",
    "- **Polynomial Regression:** Models a nonlinear relationship between \\( x \\) and \\( y \\) using polynomial terms.\n",
    "\n",
    "*Example:*\n",
    "Predicting house prices based on the size of the house, where the relationship is not linear.\n",
    "\n",
    "```python\n",
    "# Example of Polynomial Regression\n",
    "# Generate some data\n",
    "np.random.seed(0)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + X**2 + np.random.randn(100, 1)\n",
    "\n",
    "# Transform to polynomial features\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "# Create a linear regression model\n",
    "poly_reg = LinearRegression()\n",
    "poly_reg.fit(X_poly, y)\n",
    "\n",
    "# Predict values\n",
    "X_new = np.linspace(0, 2, 100).reshape(100, 1)\n",
    "X_new_poly = poly_features.transform(X_new)\n",
    "y_new = poly_reg.predict(X_new_poly)\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(X, y, color='blue', label='Actual data')\n",
    "plt.plot(X_new, y_new, color='red', label='Polynomial fit')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Polynomial Regression')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e39ca1-0061-4299-bb27-b6913b506f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Q8: Advantages and disadvantages of polynomial regression compared to linear regression. In what situations would you prefer to use polynomial regression?\n",
    "\n",
    "```markdown\n",
    "# Q8: Advantages and Disadvantages of Polynomial Regression\n",
    "\n",
    "**Advantages:**\n",
    "1. **Flexibility:** Can model nonlinear relationships.\n",
    "2. **Better Fit:** Can provide a better fit for certain datasets where the relationship is not linear.\n",
    "\n",
    "**Disadvantages:**\n",
    "1. **Overfitting:** High-degree polynomials can overfit the data.\n",
    "2. **Complexity:** More complex than linear regression, especially for high-degree polynomials.\n",
    "3. **Extrapolation:** Poor at extrapolating outside the range of the data.\n",
    "\n",
    "**When to Use Polynomial Regression:**\n",
    "1. When the relationship between the independent and dependent variables is nonlinear.\n",
    "2. When a simple linear model does not provide a good fit for the data.\n",
    "3. When prior knowledge or exploratory data analysis suggests a polynomial relationship.\n",
    "\n",
    "*Example:*\n",
    "Use polynomial regression when modeling the growth of a plant, where the growth rate accelerates or decelerates over time.\n",
    "\n",
    "```python\n",
    "# Example: Overfitting with high-degree polynomial\n",
    "# Generate some data\n",
    "np.random.seed(0)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + X**2 + np.random.randn(100, 1)\n",
    "\n",
    "# Transform to high-degree polynomial features\n",
    "poly_features_high = PolynomialFeatures(degree=10, include_bias=False)\n",
    "X_poly_high = poly_features_high.fit_transform(X)\n",
    "\n",
    "# Create a linear regression model\n",
    "poly_reg_high = LinearRegression()\n",
    "poly_reg_high.fit(X_poly_high, y)\n",
    "\n",
    "# Predict values\n",
    "X_new = np.linspace(0, 2, 100).reshape(100, 1)\n",
    "X_new_poly_high = poly_features_high.transform(X_new)\n",
    "y_new_high = poly_reg_high.predict(X_new_poly_high)\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(X, y, color='blue', label='Actual data')\n",
    "plt.plot(X_new, y_new_high, color='red', label='High-degree Polynomial fit')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('High-degree Polynomial Regression (Overfitting)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
