{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a220197-1cff-4e80-a688-644493a4dc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. An ensemble technique in machine learning refers to a method that combines predictions from multiple individual models to produce a final prediction. These individual models, often referred to as base learners or weak learners, can be of the same type (homogeneous ensemble) or different types (heterogeneous ensemble).\n",
    "\n",
    "# Q2. Ensemble techniques are used in machine learning for several reasons:\n",
    "# - They can improve predictive performance by reducing variance, bias, or both, leading to more robust and accurate models.\n",
    "# - They can mitigate overfitting by combining the predictions of multiple models trained on different subsets of data or using different algorithms.\n",
    "# - They can capture complex patterns in the data that may be missed by individual models, leading to better generalization.\n",
    "\n",
    "# Q3. Bagging, or Bootstrap Aggregating, is an ensemble technique where multiple base learners are trained on different bootstrap samples of the training data and their predictions are combined through averaging (for regression) or voting (for classification) to make the final prediction. Bagging helps reduce variance and improve stability by reducing the influence of outliers and overfitting.\n",
    "\n",
    "# Q4. Boosting is an ensemble technique where base learners are trained sequentially, with each subsequent learner focusing on the examples that were misclassified by previous learners. Boosting aims to improve the performance of weak learners by giving more weight to misclassified examples, thereby reducing bias and improving overall accuracy.\n",
    "\n",
    "# Q5. The benefits of using ensemble techniques include:\n",
    "# - Improved predictive performance: Ensemble methods often outperform individual models by reducing variance, bias, or both, leading to more accurate predictions.\n",
    "# - Robustness: Ensemble methods are less susceptible to overfitting and can better handle noisy or incomplete data.\n",
    "# - Versatility: Ensemble methods can be applied to a wide range of machine learning tasks and are compatible with various types of base learners and algorithms.\n",
    "\n",
    "# Q6. While ensemble techniques generally tend to perform better than individual models, there are cases where they may not always lead to improvement:\n",
    "# - If the base learners are highly correlated or if there is little diversity among them, ensemble methods may not provide significant benefits.\n",
    "# - Ensemble methods may also introduce additional complexity and computational overhead, which may not be justified for simple or small datasets.\n",
    "\n",
    "# Q7. The confidence interval calculated using bootstrap involves resampling the original dataset with replacement to create multiple bootstrap samples. For each bootstrap sample, the parameter of interest (e.g., mean height of trees) is estimated. The confidence interval is then constructed based on the distribution of these parameter estimates.\n",
    "\n",
    "# Q8. Bootstrap is a resampling technique used to estimate the sampling distribution of a statistic by repeatedly sampling from the original dataset with replacement. The steps involved in bootstrap are as follows:\n",
    "# 1. Randomly draw a sample (bootstrap sample) of the same size as the original dataset from the original dataset with replacement.\n",
    "# 2. Calculate the statistic of interest (e.g., mean, standard deviation) for the bootstrap sample.\n",
    "# 3. Repeat steps 1 and 2 a large number of times (typically thousands of times) to create multiple bootstrap samples and estimate the sampling distribution of the statistic.\n",
    "# 4. Construct the confidence interval using the distribution of the statistic estimates obtained from the bootstrap samples.\n",
    "\n",
    "# Q9. To estimate the 95% confidence interval for the population mean height using bootstrap:\n",
    "# Given: Sample mean height (x̄) = 15 meters, Sample standard deviation (σ) = 2 meters, Sample size (n) = 50\n",
    "# We will use the formula for the standard error of the mean (SEM) to calculate the margin of error:\n",
    "# SEM = σ / sqrt(n)\n",
    "# Margin of error = 1.96 * SEM (for a 95% confidence interval)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Given data\n",
    "sample_mean = 15\n",
    "sample_std = 2\n",
    "sample_size = 50\n",
    "confidence_level = 0.95\n",
    "\n",
    "# Calculate standard error of the mean (SEM)\n",
    "SEM = sample_std / np.sqrt(sample_size)\n",
    "\n",
    "# Calculate margin of error\n",
    "margin_of_error = 1.96 * SEM  # For 95% confidence interval\n",
    "\n",
    "# Calculate lower and upper bounds of the confidence interval\n",
    "lower_bound = sample_mean - margin_of_error\n",
    "upper_bound = sample_mean + margin_of_error\n",
    "\n",
    "print(\"95% Confidence Interval for the population mean height:\")\n",
    "print(\"Lower Bound:\", lower_bound)\n",
    "print(\"Upper Bound:\", upper_bound)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
